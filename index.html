<html>
    
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
            <title>Generative AI</title>
            <link rel="stylesheet" title="PM CSS" href="mittal.css" type="text/css">
                <meta name="google-site-verification" content="PJjpG3rijgDtqroGhZqVsZQNtp_HBJhhAJvNPKAaD9Q" />
                <!--<meta name="google-site-verification" content="sEN-WBhU3XlP0o6YEEMTBH8-86q-Vs0ELdauKy_El3o" />-->
               
                <style>
                    table, th, td {
                      border: 1px solid black;
                      border-collapse: collapse;
                    }

                .content {
                  max-width: 1000px;
                  margin: auto;
                  background: white;
                  padding: 10px;
                }
                </style>

    </head>
    
    
    <body bgcolor="white">
        <div class="content">



<h1><center>
    <br>
    ECE 590: Generative AI: Foundations, Applications, and Safety (Spring 2025)
</center></h1>
<hr>
<h2>
Instructor
</h2>
Neil Gong<br>

neil.gong@duke.edu<br>


<h2>
Teaching Assistant
</h2>
Yuqi Jia<br>

yuqi.jia@duke.edu <br>



<h2>
Lectures
</h2>
Time: MoWe 3:05PM - 4:20PM.<br>
Location: Hudson Hall 115A

<h2>
Office Hours
</h2>
Time: Tursday 9:00AM - 10:00AM.<br>
Location: 413 Wilkinson Building

<br>
<h2>
    Tentative Schedule
    </h2>

    01/08 &nbsp;&nbsp; <strong> Course overview (<a href="Lecture1.pdf">Slides</a>)</strong>

   <!-- <h3>Part I: Image generation</h3> -->

   <br><br> 01/13 &nbsp;&nbsp; <strong> Transformer </strong>
    <ul>
        <li><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></li>
    </ul>


    01/15 &nbsp;&nbsp; <strong> VAE and CLIP </strong>
    <ul>
        <li><a href="https://arxiv.org/abs/2103.00020">Learning Transferable Visual Models From Natural Language Supervision</a></li>
        <li><a href="https://arxiv.org/abs/1312.6114">Auto-Encoding Variational Bayes</a></li>

    </ul>

     01/20 &nbsp;&nbsp; <strong> Holiday </strong>
     <ul>
     </ul>


     01/22 &nbsp;&nbsp; <strong> Image generation  </strong>
     <ul>
         <li><a href="https://arxiv.org/pdf/2112.10752">High-Resolution Image Synthesis with Latent Diffusion Models</a></li>
         <li><a href="https://arxiv.org/abs/2207.12598">Classifier-Free Diffusion Guidance</a></li>
         <li>Optional: <a href="https://arxiv.org/abs/2404.02905">Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction</a></li>
         <li>Optional: <a href="https://arxiv.org/abs/2210.09276">Imagic: Text-Based Real Image Editing with Diffusion Models</a></li>

     </ul>



    01/27 &nbsp;&nbsp; <strong> Safety guardrails for image generation models (<a href="Lecture4.pdf">Slides</a>) </strong>
    <ul>
        <li><a href="https://arxiv.org/abs/2303.07345">Erasing Concepts from Diffusion Models</a></li>
        <li><a href="https://arxiv.org/abs/2404.06666">SafeGen: Mitigating Sexually Explicit Content Generation in Text-to-Image Models</a></li>
        <li>Optional: <a href="https://arxiv.org/abs/2211.05105">Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models</a></li>

    </ul>

    01/29 &nbsp;&nbsp; <strong> Jailbreaking safety guardrails of image generation models (<a href="Lecture5.pdf">Slides</a>)</strong>
    <ul>
        <li><a href="https://arxiv.org/abs/2305.12082">SneakyPrompt: Jailbreaking Text-to-image Generative Models</a></li>
        <li><a href="https://arxiv.org/abs/2310.10012">Ring-A-Bell! How Reliable are Concept Removal Methods for Diffusion Models?</a></li>
    </ul>


    02/03 &nbsp;&nbsp; <strong> AI-generated image detection: passive methods (<a href="Lecture6.pdf">Slides</a>) </strong>
    <ul>

        <li><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ojha_Towards_Universal_Fake_Image_Detectors_That_Generalize_Across_Generative_Models_CVPR_2023_paper.pdf">Towards Universal Fake Image Detectors that Generalize Across Generative Models</a></li>
        <li><a href="https://arxiv.org/abs/2003.08685">Leveraging Frequency Analysis for Deep Fake Image Recognition</a></li>
    </ul>

     02/05 &nbsp;&nbsp; <strong> AI-generated image detection: watermarks (<a href="Lecture7.pdf">Slides</a>) </strong>
     <ul>

         <li><a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper.pdf">HiDDeN: Hiding Data With Deep Networks</a></li>
         <li><a href="https://arxiv.org/abs/2305.20030">Tree-Ring Watermarks: Fingerprints for Diffusion Images that are Invisible and Robust</a></li>
         <li>Optional: <a href="https://arxiv.org/abs/2303.15435"> The Stable Signature: Rooting Watermarks in Latent Diffusion Models</a></li>
         <li>Optional: <a href="https://arxiv.org/abs/2404.04254"> Watermark-based Attribution of AI-Generated Content</a></li>

     </ul>



     02/10 &nbsp;&nbsp; <strong> Robustness of AI-generated image detectors (<a href="Lecture8.pdf">Slides</a>) </strong>
     <ul>
         <li><a href="https://urldefense.com/v3/__https://arxiv.org/abs/1706.06083__;!!OToaGQ!7aem45a2mLjp5J1EL1P2lSk_t64uHLTVkUB0kjNmypHIgNc3NmlS5W0bPiV3JDOf6qU$">Towards Deep Learning Models Resistant to Adversarial Attacks</a></li>
         <li><a href="https://arxiv.org/abs/2305.03807">Evading Watermark based Detection of AI-Generated Content</a></li>
         <li>Optional: <a href="https://arxiv.org/abs/2403.15365">A Transfer Attack to Image Watermarks</a></li>
     </ul>



     02/12 &nbsp;&nbsp; <strong> Robust AI-generated image detectors (<a href="Lecture9.pdf">Slides</a>) </strong>
     <ul>
         <li><a href="https://arxiv.org/abs/2407.04086">Certifiably Robust Image Watermark</a></li>
         <li>Optional: <a href="https://urldefense.com/v3/__https://arxiv.org/abs/1902.02918__;!!OToaGQ!7aem45a2mLjp5J1EL1P2lSk_t64uHLTVkUB0kjNmypHIgNc3NmlS5W0bPiV3msTnZzw$">Certified Adversarial Robustness via Randomized Smoothing</a></li>
     </ul>


     02/17 &nbsp;&nbsp; <strong> LLM pre-training and alignment (<a href="Lecture10.pdf">Slides</a>)  </strong>
     <ul>
         <li><a href="https://arxiv.org/abs/1909.08593">Fine-Tuning Language Models from Human Preferences</a></li>
         <li><a href="https://arxiv.org/abs/2305.18290">Direct Preference Optimization: Your Language Model is Secretly a Reward Model</a></li>
         <li>Optional: Multi-modal LLM pre-training and alignment</a></li>
         <li><a href="https://arxiv.org/abs/2301.12597">Optional: BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</a></li>
     </ul>



     02/19 &nbsp;&nbsp; <strong> LLM agent  (<a href="Lecture11.pdf">Slides</a>) </strong>
     <ul>
         <li><a href="https://arxiv.org/abs/2201.11903">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a></li>
         <li><a href="https://arxiv.org/abs/2210.03629">ReAct: Synergizing Reasoning and Acting in Language Models</a></li>
         
     </ul>


     02/24 &nbsp;&nbsp; <strong> Prompt injection attacks (<a href="Lecture12.pdf">Slides</a>) </strong>
     <ul>
         <li><a href="https://arxiv.org/abs/2310.12815">Formalizing and Benchmarking Prompt Injection Attacks and Defenses</a></li>
     </ul>


     02/26 &nbsp;&nbsp; <strong> Defenses against prompt injection attacks (<a href="Lecture13.pdf">Slides</a>) </strong>
    <ul>
        <li><a href="https://arxiv.org/abs/2402.06363">StruQ: Defending Against Prompt Injection with Structured Queries</a></li>
        <li><a href="https://arxiv.org/abs/2410.05451">Aligning LLMs to Be Robust Against Prompt Injection</a></li>
        <li>Optional: <a href="https://arxiv.org/abs/2312.17673">Jatmo: Prompt Injection Defense by Task-Specific Finetuning</a></li>
    </ul>

    03/03 &nbsp;&nbsp; <strong> Jailbreak attacks to LLM</strong>
    <ul>
        <li><a href="https://arxiv.org/abs/2307.15043">Universal and Transferable Adversarial Attacks on Aligned Language Models</a></li>
        <li><a href="https://arxiv.org/abs/2312.02119">Tree of Attacks: Jailbreaking Black-Box LLMs Automatically</a></li>
        <li>Optional: <a href="https://arxiv.org/abs/2310.08419">Jailbreaking Black Box Large Language Models in Twenty Queries</a></li>
    </ul>


     03/05 &nbsp;&nbsp; <strong>  Defenses against jailbreak attacks (<a href="Lecture14.pdf">Slides</a>)  </strong>
    <ul>
        <li><a href="https://arxiv.org/abs/2406.05946">Safety Alignment Should Be Made More Than Just a Few Tokens Deep</a></li>
        <li>Optional: <a href="https://arxiv.org/abs/2402.13494">GradSafe: Detecting Jailbreak Prompts for LLMs via Safety-Critical Gradient Analysis</a></li>
        <li>Optional: <a href="https://arxiv.org/abs/2402.08983">SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding</a></li>
    </ul>


     03/10 &nbsp;&nbsp; <strong> Spring recess  </strong>
     <ul>
     </ul>


    03/12 &nbsp;&nbsp; <strong> Spring recess</strong>
    <ul>
    </ul>



     03/17 &nbsp;&nbsp; <strong> AI-generated text detection: passive detectors </strong>
     <ul>
         <li><a href="https://proceedings.mlr.press/v202/mitchell23a.html">DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature</a></li>
     </ul>


     
     03/19 &nbsp;&nbsp; <strong> AI-generated text detection: watermarks (<a href="Lecture15.pdf">Slides</a>)  </strong>
    <ul>
        <li><a href="https://arxiv.org/abs/2301.10226">A Watermark for Large Language Models</a></li>
        <li><a href="https://arxiv.org/abs/2009.03015">Adversarial Watermarking Transformer: Towards Tracing Text Provenance with Data Hiding</a></li>
        <li>Optional: <a href="https://www.nature.com/articles/s41586-024-08025-4">Scalable watermarking for identifying large language model outputs</a></li>
    </ul>



     03/24 &nbsp;&nbsp; <strong> Robustness of AI-generated text detectors (<a href="Lecture15.pdf">Slides</a>)  </strong>
     <ul>
         <li><a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/575c450013d0e99e4b0ecf82bd1afaa4-Abstract-Conference.html">Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense</a></li>
 
     </ul>


     03/26 &nbsp;&nbsp; <strong> Hallucination (<a href="Lecture16.pdf">Slides</a>)  </strong>
     <ul>
         <li><a href="https://arxiv.org/abs/2310.06271">Towards Mitigating Hallucination in Large Language Models via Self-Reflection</a></li>
         <li><a href="https://arxiv.org/abs/2402.14683">Visual Hallucinations of Multi-modal Large Language Models</a></li>
     </ul>
     
     03/31 &nbsp;&nbsp; <strong> Data-use auditing: passive methods (<a href="Lecture17.pdf">Slides</a>) </strong>
     <ul>
         <li><a href="https://urldefense.com/v3/__https://arxiv.org/abs/1610.05820__;!!OToaGQ!7aem45a2mLjp5J1EL1P2lSk_t64uHLTVkUB0kjNmypHIgNc3NmlS5W0bPiV3QBw3Kgo$">Membership Inference Attacks against Machine Learning Models</a></li>
         <li><a href="https://arxiv.org/abs/2310.16789">Detecting Pretraining Data from Large Language Models</a></li>
         <li>Optional: <a href="https://arxiv.org/abs/2112.03570">Membership Inference Attacks From First Principles</a></li>

     </ul>


      04/02 &nbsp;&nbsp; <strong> Data-use auditing: proactive methods (<a href="Lecture18.pdf">Slides</a>) </strong>
      <ul>
          <li><a href="https://arxiv.org/abs/2407.15100">A General Framework for Data-Use Auditing of ML Models</a></li>
          <li>Optional: <a href="https://urldefense.com/v3/__https://arxiv.org/pdf/2002.00937.pdf__;!!OToaGQ!6F-a7tW7vP82pAboe34Vfvy2Vh-v7TG-8DfDiy7lSypZ4E9wi_B_-hNY8NL4pBqAr78$">Radioactive data: tracing through training</a></li>
      </ul>

     04/07 &nbsp;&nbsp; <strong> Audio generation and safety issues (<a href="Lecture19.pdf">Slides</a>) </strong>
     <ul>
         <li><a href="https://arxiv.org/abs/2301.12503">Audioldm: Text-to-audio generation with latent diffusion models</a></li>
         <li>Optional: <a href="https://arxiv.org/abs/2401.17264">Proactive Detection of Voice Cloning with Localized Watermarking</a></li>
     </ul>


     04/09 &nbsp;&nbsp; <strong> Video generation and safety issues (<a href="Lecture20.pdf">Slides</a>) </strong>
     <ul>
         <li><a href="https://arxiv.org/abs/2209.14792">Make-a-video: Text-to-video generation without text-video data</a></li>
         <li><a href="http://openaccess.thecvf.com/content/CVPR2022/html/Hu_Make_It_Move_Controllable_Image-to-Video_Generation_With_Text_Descriptions_CVPR_2022_paper.html">Make it move: controllable image-to-video generation with text descriptions</a></li>
         <li>Optional: <a href="https://arxiv.org/abs/2412.09122">LVMark: Robust Watermark for latent video diffusion models</a></li>
     </ul>
     
     04/14 &nbsp;&nbsp; <strong> Project presentation </strong>
     <ul>
     </ul>



     04/16 &nbsp;&nbsp; <strong> Project presentation  </strong>
    <ul>
    </ul>


<h2>
Prerequisite
</h2>

ECE 580 or 687D or Computer Science 371 or graduate standing.


<h2>
Course Description
</h2>
Generative AI is revolutionizing content creation by enabling machines to generate text, images, videos, music, and even code. In this course, we will discuss foundations, applications, and safety and security of generative AI.

<h2>
Class Format
</h2>
The class is structured around paper reading, lectures, discussions, and projects. Each lecture will focus on a specific topic, with students expected to read the suggested papers and submit their comments to a designated email address by the end of the day before the lecture. Students will be required to lead a lecture on a chosen topic, complete a class project, present their project, and write a project report. Groups of up to three students can be formed for both the lecture and the class project.
<h2>
Deadlines
</h2>
Reading assignments
<ul>
    <li>Sunday and Tuesday 11:59pm. Send comments to adversarialmlduke@gmail.com. Please send your comments to all papers in a single email thread.</li>
</ul>

Choosing a topic for lecture
<ul>
    <li>A group sends three preferred dates to adversarialmlduke@gmail.com by 11:59pm, 01/25.</li>
</ul>

Class project
<ul>
    <li>02/01: project proposal due.</li>
    <li>03/15: milestone report due.</li>
    <li>04/14, 04/16: project presentation.</li>
    <li>04/27: final project report due.</li>
</ul>


<h2>
Grading Policy
</h2>
50% project<br>
25% reading assignment<br>
10% class participation<br>
15% class presentation

    <br>
                        <br>
                        
                        <br>
                        <br>
                        
     </div>
    </body>
</html>

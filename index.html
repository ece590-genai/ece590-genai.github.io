<html>
    
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
            <title>Generative AI</title>
            <link rel="stylesheet" title="PM CSS" href="mittal.css" type="text/css">
                <meta name="google-site-verification" content="PJjpG3rijgDtqroGhZqVsZQNtp_HBJhhAJvNPKAaD9Q" />
                <!--<meta name="google-site-verification" content="sEN-WBhU3XlP0o6YEEMTBH8-86q-Vs0ELdauKy_El3o" />-->
               
                <style>
                    table, th, td {
                      border: 1px solid black;
                      border-collapse: collapse;
                    }

                .content {
                  max-width: 1000px;
                  margin: auto;
                  background: white;
                  padding: 10px;
                }
                </style>

    </head>
    
    
    <body bgcolor="white">
        <div class="content">



<h1><center>
    <br>
    ECE 590: Generative AI: Foundations, Applications, and Safety (Spring 2026)
</center></h1>
<hr>
<h2>
Instructor
</h2>
Neil Gong, neil.gong@duke.edu<br>


<h2>
Teaching Assistant
</h2>
Reachal Wang, reachal.wang@duke.edu <br>
Jason Wang, xilong.wang@duke.edu <br>



<h2>
Lectures
</h2>
Time: MoWe 3:05PM - 4:20PM.<br>
Location: Teer 203

<h2>
Office Hours
</h2>
Time: Wed. 11:00AM - 11:50AM.<br>
Location: 413 Wilkinson Building

<br>
<h2>
    Tentative Schedule
</h2>

01/07 &nbsp;&nbsp; <strong> Overview </strong> [<a href="slides/Lecture_1.pdf">PDF</a>] <br><br>

01/12 &nbsp;&nbsp; <strong> Transformer </strong> [<a href="slides/Lecture-2-3.pdf">PDF</a>]
<ul>
    <li><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></li>
</ul>

01/14 &nbsp;&nbsp; <strong> LLM pre-training and post-training </strong> [<a href="slides/Lecture-4.pdf">PDF</a>]
<ul>
    <li><a href="https://arxiv.org/abs/1909.08593">Fine-Tuning Language Models from Human Preferences</a></li>
    <li><a href="https://arxiv.org/abs/2305.18290">Direct Preference Optimization: Your Language Model is Secretly a Reward Model</a></li>
    <li>Optional: <a href="https://arxiv.org/abs/2501.12948">DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</a></li>
</ul>

01/19 &nbsp;&nbsp; <strong> Holiday. No class </strong>
<ul>
</ul>

01/21 &nbsp;&nbsp; <strong> LLM agent </strong> [<a href="slides/Lecture-4.pdf">PDF</a>]
<ul>
    <li><a href="https://arxiv.org/abs/2201.11903">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a></li>
    <li><a href="https://arxiv.org/abs/2210.03629">ReAct: Synergizing Reasoning and Acting in Language Models</a></li>
</ul>

01/26 &nbsp;&nbsp; <strong> Prompt injection attacks </strong>
<ul>
    <li><a href="https://arxiv.org/abs/2310.12815">Formalizing and Benchmarking Prompt Injection Attacks and Defenses</a></li>
</ul>

01/28 &nbsp;&nbsp; <strong> Fine-tuning LLMs to be secure against prompt injection attacks </strong>
<ul>
    <li><a href="https://arxiv.org/abs/2402.06363">StruQ: Defending Against Prompt Injection with Structured Queries</a></li>
    <li><a href="https://arxiv.org/abs/2410.05451">SecAlign: Defending Against Prompt Injection with Preference Optimization</a></li>
    <li>Optional: <a href="https://arxiv.org/abs/2507.02735">Meta SecAlign: A Secure Foundation LLM Against Prompt Injection Attacks</a></li>
</ul>

02/02 &nbsp;&nbsp; <strong> Detecting and localizing prompt injection attacks </strong>
<ul>
    <li><a href="https://arxiv.org/abs/2504.11358">DataSentinel: A Game-Theoretic Detection of Prompt Injection Attacks</a></li>
    <li><a href="https://arxiv.org/abs/2510.12252">PromptLocate: Localizing Prompt Injection Attacks</a></li>
</ul>

02/04 &nbsp;&nbsp; <strong> Adaptive prompt injection attacks </strong>
<ul>
    <li><a href="https://arxiv.org/abs/2510.09023">The Attacker Moves Second: Stronger Adaptive Attacks Bypass Defenses Against Llm Jailbreaks and Prompt Injections</a></li>
    <li>Optional: <a href="https://arxiv.org/abs/2505.18333">A Critical Evaluation of Defenses against Prompt Injection Attacks</a></li>
</ul>

02/09 &nbsp;&nbsp; <strong> Jailbreak attacks to LLM </strong>
<ul>
    <li><a href="https://arxiv.org/abs/2307.15043">Universal and Transferable Adversarial Attacks on Aligned Language Models</a></li>
    <li><a href="https://arxiv.org/abs/2312.02119">Tree of Attacks: Jailbreaking Black-Box LLMs Automatically</a></li>
    <li>Optional: <a href="https://arxiv.org/abs/2310.08419">Jailbreaking Black Box Large Language Models in Twenty Queries</a></li>
</ul>

02/11 &nbsp;&nbsp; <strong> Defenses against jailbreak attacks </strong>
<ul>
    <li><a href="https://arxiv.org/abs/2406.05946">Safety Alignment Should Be Made More Than Just a Few Tokens Deep</a></li>
    <li>Optional: <a href="https://arxiv.org/abs/2309.00614">Baseline Defenses for Adversarial Attacks Against Aligned Language Models</a></li>
    <li>Optional: <a href="https://arxiv.org/abs/2402.08983">SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding</a></li>
</ul>

02/16 &nbsp;&nbsp; <strong> AI-generated text detection: passive detectors </strong>
<ul>
    <li><a href="https://proceedings.mlr.press/v202/mitchell23a.html">DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature</a></li>
</ul>

02/18 &nbsp;&nbsp; <strong> AI-generated text detection: watermarks </strong>
<ul>
    <li><a href="https://arxiv.org/abs/2301.10226">A Watermark for Large Language Models</a></li>
    <li><a href="https://arxiv.org/abs/2009.03015">Adversarial Watermarking Transformer: Towards Tracing Text Provenance with Data Hiding</a></li>
    <li>Optional: <a href="https://www.nature.com/articles/s41586-024-08025-4">Scalable watermarking for identifying large language model outputs</a></li>
</ul>

02/23 &nbsp;&nbsp; <strong> Robustness of AI-generated text detectors </strong>
<ul>
    <li><a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/575c450013d0e99e4b0ecf82bd1afaa4-Abstract-Conference.html">Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense</a></li>
</ul>

02/25 &nbsp;&nbsp; <strong> VAE, Dino, and CLIP </strong>
<ul>
    <li><a href="https://arxiv.org/abs/2103.00020">Learning Transferable Visual Models From Natural Language Supervision</a></li>
    <li><a href="https://arxiv.org/abs/1312.6114">Auto-Encoding Variational Bayes</a></li>
    <li>Optional: <a href="https://scontent-atl3-1.xx.fbcdn.net/v/t39.2365-6/531524719_1692810264763997_2330122477414087224_n.pdf?_nc_cat=103&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=HQ_DfbahNhMQ7kNvwHA_jhu&_nc_oc=AdlcWuvckVcpKYoBjpp37mUlx7AESbPnjs9EYN1q5DNS6sOY23xb5LE2SzsUrCEEUdNGpPzwcd8OIgPxFvx6Hv7g&_nc_zt=14&_nc_ht=scontent-atl3-1.xx&_nc_gid=bNyv2GCOFESeOIcN9QCE2A&oh=00_AfquuGxTAOyPTRrOITeySPemvWVAvP6n8GHHu4hTmzKuYA&oe=69618F68">DINOv3</a></li>
</ul>

03/02 &nbsp;&nbsp; <strong> Image generation </strong>
<ul>
    <li><a href="https://arxiv.org/pdf/2112.10752">High-Resolution Image Synthesis with Latent Diffusion Models</a></li>
    <li><a href="https://arxiv.org/abs/2207.12598">Classifier-Free Diffusion Guidance</a></li>
    <li>Optional: <a href="https://arxiv.org/abs/2404.02905">Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction</a></li>
    <li>Optional: <a href="https://arxiv.org/abs/2210.09276">Imagic: Text-Based Real Image Editing with Diffusion Models</a></li>
</ul>

03/04 &nbsp;&nbsp; <strong> Safety guardrails for image generation models </strong>
<ul>
    <li><a href="https://arxiv.org/abs/2303.07345">Erasing Concepts from Diffusion Models</a></li>
    <li><a href="https://arxiv.org/abs/2404.06666">SafeGen: Mitigating Sexually Explicit Content Generation in Text-to-Image Models</a></li>
    <li>Optional: <a href="https://arxiv.org/abs/2211.05105">Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models</a></li>
</ul>

03/09 &nbsp;&nbsp; <strong> Spring recess </strong>
<ul>
</ul>

03/11 &nbsp;&nbsp; <strong> Spring recess </strong>
<ul>
</ul>

03/16 &nbsp;&nbsp; <strong> Jailbreaking safety guardrails of image generation models </strong>
<ul>
    <li><a href="https://arxiv.org/abs/2305.12082">SneakyPrompt: Jailbreaking Text-to-image Generative Models</a></li>
    <li><a href="https://arxiv.org/abs/2310.10012">Ring-A-Bell! How Reliable are Concept Removal Methods for Diffusion Models?</a></li>
</ul>

03/18 &nbsp;&nbsp; <strong> AI-generated image detection: passive methods </strong>
<ul>
    <li><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ojha_Towards_Universal_Fake_Image_Detectors_That_Generalize_Across_Generative_Models_CVPR_2023_paper.pdf">Towards Universal Fake Image Detectors that Generalize Across Generative Models</a></li>
    <li><a href="https://arxiv.org/abs/2003.08685">Leveraging Frequency Analysis for Deep Fake Image Recognition</a></li>
</ul>

03/23 &nbsp;&nbsp; <strong> AI-generated image detection: watermarks </strong>
<ul>
    <li><a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper.pdf">HiDDeN: Hiding Data With Deep Networks</a></li>
    <li><a href="https://arxiv.org/abs/2305.20030">Tree-Ring Watermarks: Fingerprints for Diffusion Images that are Invisible and Robust</a></li>
    <li>Optional: <a href="https://arxiv.org/abs/2303.15435">The Stable Signature: Rooting Watermarks in Latent Diffusion Models</a></li>
    <li>Optional: <a href="https://arxiv.org/abs/2404.04254/">Watermark-based Attribution of AI-Generated Content</a></li>
</ul>

03/25 &nbsp;&nbsp; <strong> Robustness of AI-generated image detectors </strong>
<ul>
    <li><a href="https://urldefense.com/v3/__https://arxiv.org/abs/1706.06083__;!!OToaGQ!7aem45a2mLjp5J1EL1P2lSk_t64uHLTVkUB0kjNmypHIgNc3NmlS5W0bPiV3JDOf6qU$">Towards Deep Learning Models Resistant to Adversarial Attacks</a></li>
    <li><a href="https://arxiv.org/abs/2305.03807">Evading Watermark based Detection of AI-Generated Content</a></li>
    <li>Optional: <a href="https://arxiv.org/abs/2403.15365">A Transfer Attack to Image Watermarks</a></li>
</ul>

03/30 &nbsp;&nbsp; <strong> Data-use auditing: passive methods </strong>
<ul>
    <li><a href="https://urldefense.com/v3/__https://arxiv.org/abs/1610.05820__;!!OToaGQ!7aem45a2mLjp5J1EL1P2lSk_t64uHLTVkUB0kjNmypHIgNc3NmlS5W0bPiV3QBw3Kgo$">Membership Inference Attacks against Machine Learning Models</a></li>
    <li><a href="https://arxiv.org/abs/2310.16789">Detecting Pretraining Data from Large Language Models</a></li>
    <li>Optional: <a href="https://arxiv.org/abs/2112.03570">Membership Inference Attacks From First Principles</a></li>
</ul>

04/01 &nbsp;&nbsp; <strong> Data-use auditing: proactive methods </strong>
<ul>
    <li><a href="https://arxiv.org/abs/2407.15100">A General Framework for Data-Use Auditing of ML Models</a></li>
    <li>Optional: <a href="https://urldefense.com/v3/__https://arxiv.org/pdf/2002.00937.pdf__;!!OToaGQ!6F-a7tW7vP82pAboe34Vfvy2Vh-v7TG-8DfDiy7lSypZ4E9wi_B_-hNY8NL4pBqAr78$">Radioactive data: tracing through training</a></li>
</ul>

04/06 &nbsp;&nbsp; <strong> Audio generation and safety issues </strong>
<ul>
    <li><a href="https://arxiv.org/abs/2301.12503">Audioldm: Text-to-audio generation with latent diffusion models</a></li>
    <li>Optional: <a href="https://arxiv.org/abs/2401.17264">Proactive Detection of Voice Cloning with Localized Watermarking</a></li>
</ul>

04/08 &nbsp;&nbsp; <strong> Video generation and safety issues </strong>
<ul>
    <li><a href="https://arxiv.org/abs/2209.14792">Make-a-video: Text-to-video generation without text-video data</a></li>
    <li><a href="http://openaccess.thecvf.com/content/CVPR2022/html/Hu_Make_It_Move_Controllable_Image-to-Video_Generation_With_Text_Descriptions_CVPR_2022_paper.html">Make it move: controllable image-to-video generation with text descriptions</a></li>
    <li>Optional: <a href="https://ieeexplore.ieee.org/abstract/document/10086041/?casa_token=8ymINcfwwgcAAAAA:vjqHNF84_y3J9E_ACom5IyXcjWVbbLKC9s8auyiLs3MOsfd9THd30JKSc0VKa1mSLBl3lvP9QKo">DVMark: a deep multiscale framework for video watermarking</a></li>
</ul>

04/13 &nbsp;&nbsp; <strong> Project presentation </strong>
<ul>
</ul>

04/15 &nbsp;&nbsp; <strong> Project presentation </strong>
<ul>
</ul>


<h2>
Prerequisite
</h2>

ECE 580 or 687D or Computer Science 371 or graduate standing.


<h2>
Course Description
</h2>
Generative AI is revolutionizing content creation by enabling machines to generate text, images, videos, music, and even code. In this course, we will discuss foundations, applications, and safety and security of generative AI.

<h2>
Class Format
</h2>
The class is structured around paper reading, lectures, discussions, and projects. Each lecture will focus on a specific topic, with students expected to read the suggested papers and submit their comments to a designated email address by the end of the day before the lecture. Students will be required to lead a lecture on a chosen topic, complete a class project, present their project, and write a project report. Groups of up to four students can be formed for both the lecture and the class project.
<h2>
Deadlines
</h2>
Reading assignments
<ul>
    <li>Sunday and Tuesday 11:59pm. Send comments to ecegenerativeai@gmail.com. Please send your comments to all papers in a single email thread. Disclose any LLM use when sending comments.</li>
</ul>

Choosing a topic for lecture
<ul>
    <li>A group sends three preferred dates to ecegenerativeai@gmail.com by 11:59pm, 01/25. Only one group member sends the email on behalf of the group. Please indicate your group members in the email.</li>
</ul>

Class project
<ul>
    <li>02/01: project proposal due.</li>
    <li>03/15: milestone report due.</li>
    <li>04/13, 04/15: project presentation.</li>
    <li>04/26: final project report due.</li>
</ul>


<h2>
Grading Policy
</h2>
50% project<br>
25% reading assignment<br>
10% class participation<br>
15% class presentation

    <br>
                        <br>
                        
                        <br>
                        <br>
                        
     </div>
    </body>
</html>

<html>
    
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
            <title>Generative AI</title>
            <link rel="stylesheet" title="PM CSS" href="mittal.css" type="text/css">
                <meta name="google-site-verification" content="PJjpG3rijgDtqroGhZqVsZQNtp_HBJhhAJvNPKAaD9Q" />
                <!--<meta name="google-site-verification" content="sEN-WBhU3XlP0o6YEEMTBH8-86q-Vs0ELdauKy_El3o" />-->
               
                <style>
                    table, th, td {
                      border: 1px solid black;
                      border-collapse: collapse;
                    }

                .content {
                  max-width: 1000px;
                  margin: auto;
                  background: white;
                  padding: 10px;
                }
                </style>

    </head>
    
    
    <body bgcolor="white">
        <div class="content">



<h1><center>
    <br>
    ECE 590: Generative AI: Foundations, Applications, and Safety (Spring 2025)
</center></h1>
<hr>
<h2>
Instructor
</h2>
Neil Gong<br>

neil.gong@duke.edu<br>


<h2>
Teaching Assistant
</h2>
Yuqi Jia<br>

yuqi.jia@duke.edu <br>



<h2>
Lectures
</h2>
Time: MoWe 3:05PM - 4:20PM.<br>
Location: Hudson Hall 115A

<h2>
Office Hours
</h2>
Time: Tursday 9:00AM - 10:00AM.<br>
Location: 413 Wilkinson Building

<br>
<h2>
    Tentative Schedule
    </h2>

    01/08 &nbsp;&nbsp; <strong> Course overview (<a href="Lecture-1-overview.pdf">Slides</a>)</strong>

   <!-- <h3>Part I: Image generation</h3> -->

   <br><br> 01/13 &nbsp;&nbsp; <strong> Transformer (<a href="Lecture-2-neural-network-architecture.pdf">Slides</a>)</strong>
    <ul>
        <li><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></li>
    </ul>


    01/15 &nbsp;&nbsp; <strong> Transformer (<a href="Lecture-2-neural-network-architecture.pdf">Slides</a>)</strong>
    <ul>

    </ul>

     01/20 &nbsp;&nbsp; <strong> Holiday </strong>
     <ul>
     </ul>

     01/22 &nbsp;&nbsp; <strong> Representation learning (<a href="Lecture-3-representation-learning.pdf">Slides</a>)</strong>
     <ul>
         <li><a href="https://arxiv.org/abs/2103.00020">Learning Transferable Visual Models From Natural Language Supervision</a></li>
         <li><a href="https://arxiv.org/abs/1312.6114">Auto-Encoding Variational Bayes</a></li>

     </ul>

     
     01/27 &nbsp;&nbsp; <strong> Image generation (<a href="Lecture-4-image-generation.pdf">Slides</a>) </strong>
     <ul>
         <li><a href="https://arxiv.org/pdf/2006.11239">Denoising Diffusion Probabilistic Models</a></li>
         <li><a href="https://arxiv.org/pdf/2112.10752">High-Resolution Image Synthesis with Latent Diffusion Models</a></li>
         <li>Optional: <a href="https://arxiv.org/abs/2207.12598">Classifier-Free Diffusion Guidance</a></li>
         <li>Optional: <a href="https://arxiv.org/abs/2404.02905">Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction</a></li>
         <li>Optional: <a href="https://arxiv.org/abs/2210.09276">Imagic: Text-Based Real Image Editing with Diffusion Models</a></li>

     </ul>



    01/29 &nbsp;&nbsp; <strong> Safety guardrails for image generation models (<a href="Lecture-5-image-generation-safetyguard.pdf">Slides</a>) </strong>
    <ul>
        <li><a href="https://arxiv.org/abs/2303.07345">Erasing Concepts from Diffusion Models</a></li>
        <li>Optional: <a href="https://arxiv.org/abs/2404.06666">SafeGen: Mitigating Sexually Explicit Content Generation in Text-to-Image Models</a></li>
        <li>Optional: <a href="https://arxiv.org/abs/2211.05105">Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models</a></li>

    </ul>

    02/03 &nbsp;&nbsp; <strong> Jailbreaking safety guardrails of image generation models (<a href="Lecture-6-image-generation-jailbreak.pdf">Slides</a>)</strong>
    <ul>
        <li><a href="https://arxiv.org/abs/2305.12082">SneakyPrompt: Jailbreaking Text-to-image Generative Models</a></li>
        <li>Optional: <a href="https://arxiv.org/abs/2310.10012">Ring-A-Bell! How Reliable are Concept Removal Methods for Diffusion Models?</a></li>
        <li>Speakers: Chengyang Zhou and Michael (Zeyu) Li</li>
    </ul>

    
    02/05 &nbsp;&nbsp; <strong> AI-generated image detection (<a href="Lecture-7-image-detector.pdf">Slides</a>) </strong>
    <ul>

        <li><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ojha_Towards_Universal_Fake_Image_Detectors_That_Generalize_Across_Generative_Models_CVPR_2023_paper.pdf">Towards Universal Fake Image Detectors that Generalize Across Generative Models</a></li>
        <li><a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper.pdf">HiDDeN: Hiding Data With Deep Networks</a></li>

        <li>Optional: <a href="https://arxiv.org/abs/2003.08685">Leveraging Frequency Analysis for Deep Fake Image Recognition</a></li>
        <li>Optional: <a href="https://arxiv.org/abs/2305.20030">Tree-Ring Watermarks: Fingerprints for Diffusion Images that are Invisible and Robust</a></li>
        <li>Optional: <a href="https://arxiv.org/abs/2303.15435"> The Stable Signature: Rooting Watermarks in Latent Diffusion Models</a></li>
        <li>Optional: <a href="https://arxiv.org/abs/2404.04254"> Watermark-based Attribution of AI-Generated Content</a></li>
        <li>Speakers: Hung Anh Vu, Steven Seiden, and Zini Yang</li>
    </ul>


     02/10 &nbsp;&nbsp; <strong> Robustness of AI-generated image detectors (<a href="Lecture-8-image-detector-robustness.pdf">Slides</a>) </strong>
     <ul>
         <li><a href="https://urldefense.com/v3/__https://arxiv.org/abs/1706.06083__;!!OToaGQ!7aem45a2mLjp5J1EL1P2lSk_t64uHLTVkUB0kjNmypHIgNc3NmlS5W0bPiV3JDOf6qU$">Towards Deep Learning Models Resistant to Adversarial Attacks</a></li>
         <li><a href="https://arxiv.org/abs/2305.03807">Evading Watermark based Detection of AI-Generated Content</a></li>
         <li>Optional: <a href="https://arxiv.org/abs/2403.15365">A Transfer Attack to Image Watermarks</a></li>
         <li>Speakers: Anika Mitra and Adam Kosinski</li>
     </ul>



     02/12 &nbsp;&nbsp; <strong> Robust AI-generated image detectors (<a href="Lecture-9-robust-image-detector.pdf">Slides</a>) </strong>
     <ul>
         <li><a href="https://arxiv.org/abs/2407.04086">Certifiably Robust Image Watermark</a></li>
         <li>Optional: <a href="https://urldefense.com/v3/__https://arxiv.org/abs/1902.02918__;!!OToaGQ!7aem45a2mLjp5J1EL1P2lSk_t64uHLTVkUB0kjNmypHIgNc3NmlS5W0bPiV3msTnZzw$">Certified Adversarial Robustness via Randomized Smoothing</a></li>
     </ul>


     02/17 &nbsp;&nbsp; <strong> LLM pre-training and alignment  </strong>
     <ul>
         <li><a href="https://arxiv.org/abs/1909.08593">Fine-Tuning Language Models from Human Preferences</a></li>
         <li><a href="https://arxiv.org/abs/2305.18290">Direct Preference Optimization: Your Language Model is Secretly a Reward Model</a></li>
         <li>Optional: Multi-modal LLM pre-training and alignment</a></li>
         <li><a href="https://arxiv.org/abs/2301.12597">Optional: BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</a></li>
         <li>Speakers: Peter Yang, Mobasserul Haque, and Dhaval Potdar</li>
     </ul>



     02/19 &nbsp;&nbsp; <strong> LLM agent </strong>
     <ul>
         <li><a href="https://arxiv.org/abs/2201.11903">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a></li>
         <li><a href="https://arxiv.org/abs/2210.03629">ReAct: Synergizing Reasoning and Acting in Language Models</a></li>
         <li>Speakers: Qinsi Wang and Ming-Yu Chung</li>
     </ul>


     02/24 &nbsp;&nbsp; <strong> Prompt injection attacks </strong>
     <ul>
         <li><a href="https://arxiv.org/abs/2310.12815">Formalizing and Benchmarking Prompt Injection Attacks and Defenses</a></li>
         <li>Speakers: Jason Wang and Reachal Wang</li>
     </ul>


     02/26 &nbsp;&nbsp; <strong> Defenses against prompt injection attacks (<a href="[Talk] Prompt Injection Defenses.pdf">Slides</a>) </strong>
    <ul>
        <li><a href="https://arxiv.org/abs/2402.06363">StruQ: Defending Against Prompt Injection with Structured Queries</a></li>
        <li><a href="https://arxiv.org/abs/2410.05451">Aligning LLMs to Be Robust Against Prompt Injection</a></li>
        <li>Optional: <a href="https://arxiv.org/abs/2312.17673">Jatmo: Prompt Injection Defense by Task-Specific Finetuning</a></li>
        <li>Guest speaker: Sizhe Chen, UC Berkeley</li>
    </ul>

    03/03 &nbsp;&nbsp; <strong> Jailbreak attacks to LLM</strong>
    <ul>
        <li><a href="https://arxiv.org/abs/2307.15043">Universal and Transferable Adversarial Attacks on Aligned Language Models</a></li>
        <li><a href="https://arxiv.org/abs/2312.02119">Tree of Attacks: Jailbreaking Black-Box LLMs Automatically</a></li>
        <li>Optional: <a href="https://arxiv.org/abs/2310.08419">Jailbreaking Black Box Large Language Models in Twenty Queries</a></li>
        <li>Optional: <a href="https://arxiv.org/abs/2404.16873">AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs</a></li>
        <li>Speakers: Zhendong Zhang, Vivian Zhang, and Ming Yin</li>
    </ul>


     03/05 &nbsp;&nbsp; <strong>  Defenses against jailbreak attacks </strong>
    <ul>
        <li><a href="https://arxiv.org/abs/2406.05946">Safety Alignment Should Be Made More Than Just a Few Tokens Deep</a></li>
        <li>Optional: <a href="https://arxiv.org/abs/2402.13494">GradSafe: Detecting Jailbreak Prompts for LLMs via Safety-Critical Gradient Analysis</a></li>
        <li>Optional: <a href="https://arxiv.org/abs/2402.08983">SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding</a></li>
        <li>Speakers: Yuchen Jiang, Zedian Shao, and Yangchenchen Jin</li>
    </ul>


     03/10 &nbsp;&nbsp; <strong> Spring recess  </strong>
     <ul>
     </ul>


    03/12 &nbsp;&nbsp; <strong> Spring recess</strong>
    <ul>
    </ul>



     03/17 &nbsp;&nbsp; <strong> AI-generated text detection: passive detectors (<a href="Lecture-passive-detector-text.pdf">Slides</a>)</strong>
     <ul>
         <li><a href="https://proceedings.mlr.press/v202/mitchell23a.html">DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature</a></li>
     </ul>


     
     03/19 &nbsp;&nbsp; <strong> AI-generated text detection: watermarks (<a href="Lecture-watermark-text.pdf">Slides</a>)  </strong>
    <ul>
        <li><a href="https://arxiv.org/abs/2301.10226">A Watermark for Large Language Models</a></li>
        <li><a href="https://arxiv.org/abs/2009.03015">Adversarial Watermarking Transformer: Towards Tracing Text Provenance with Data Hiding</a></li>
        <li>Optional: <a href="https://www.nature.com/articles/s41586-024-08025-4">Scalable watermarking for identifying large language model outputs</a></li>
    </ul>



     03/24 &nbsp;&nbsp; <strong> Robustness of AI-generated text detectors (<a href="Generative AI_ Robustness of AI-Generated Text Detectors.pdf">Slides</a>) </strong>
     <ul>
         <li><a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/575c450013d0e99e4b0ecf82bd1afaa4-Abstract-Conference.html">Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense</a></li>
         <li>Optional: <a href="https://arxiv.org/abs/2406.01179">Are AI-Generated Text Detectors Robust to Adversarial Perturbations?</a></li>
         <li>Speakers: Austin Phillips, Osama Ahmed, and Ryan Devries</li>
     </ul>


     03/26 &nbsp;&nbsp; <strong> Hallucination  </strong>
     <ul>
         <li><a href="https://arxiv.org/abs/2310.06271">Towards Mitigating Hallucination in Large Language Models via Self-Reflection</a></li>
         <li><a href="https://arxiv.org/abs/2402.14683">Visual Hallucinations of Multi-modal Large Language Models</a></li>
         <li>Speakers: Yanming Xiu</li>
     </ul>
     
     03/31 &nbsp;&nbsp; <strong> Data-use auditing: passive methods </strong>
     <ul>
         <li><a href="https://urldefense.com/v3/__https://arxiv.org/abs/1610.05820__;!!OToaGQ!7aem45a2mLjp5J1EL1P2lSk_t64uHLTVkUB0kjNmypHIgNc3NmlS5W0bPiV3QBw3Kgo$">Membership Inference Attacks against Machine Learning Models</a></li>
         <li><a href="https://arxiv.org/abs/2310.16789">Detecting Pretraining Data from Large Language Models</a></li>
         <li>Optional: <a href="https://arxiv.org/abs/2112.03570">Membership Inference Attacks From First Principles</a></li>

     </ul>


      04/02 &nbsp;&nbsp; <strong> Data-use auditing: proactive methods </strong>
      <ul>
          <li><a href="https://arxiv.org/abs/2407.15100">A General Framework for Data-Use Auditing of ML Models</a></li>
          <li>Optional: <a href="https://urldefense.com/v3/__https://arxiv.org/pdf/2002.00937.pdf__;!!OToaGQ!6F-a7tW7vP82pAboe34Vfvy2Vh-v7TG-8DfDiy7lSypZ4E9wi_B_-hNY8NL4pBqAr78$">Radioactive data: tracing through training</a></li>
      </ul>

     04/07 &nbsp;&nbsp; <strong> Audio generation and safety issues </strong>
     <ul>
         <li><a href="https://arxiv.org/abs/2301.12503">Audioldm: Text-to-audio generation with latent diffusion models</a></li>
         <li>Optional: <a href="https://arxiv.org/abs/2401.17264">Proactive Detection of Voice Cloning with Localized Watermarking</a></li>
         <li>Speakers: Hao-Lun Hsu, Jiwoo Kim, and Naman Saxena</li>
     </ul>


     04/09 &nbsp;&nbsp; <strong> Video generation and safety issues </strong>
     <ul>
         <li><a href="https://arxiv.org/abs/2209.14792">Make-a-video: Text-to-video generation without text-video data</a></li>
         <li><a href="http://openaccess.thecvf.com/content/CVPR2022/html/Hu_Make_It_Move_Controllable_Image-to-Video_Generation_With_Text_Descriptions_CVPR_2022_paper.html">Make it move: controllable image-to-video generation with text descriptions</a></li>
         <li>Optional: <a href="https://arxiv.org/abs/2412.09122">LVMark: Robust Watermark for latent video diffusion models</a></li>
         <li>Speakers: Hengfan Zhang, Yupu Wang, and Haocheng Ni</li>
     </ul>
     
     04/14 &nbsp;&nbsp; <strong> Project presentation </strong>
     <ul>
        <li><a>Group 1: Hung Anh Vu</a></li>
        <li><a>Group 2: Yanming Xiu</a></li>
        <li><a>Group 3: Austin Phillips</a></li>
        <li><a>Group 4: Mobasserul Haque and Dhaval Potdar</a></li>
        <li><a>Group 5: Zini Yang</a></li>
        <li><a>Group 6: Zhendong Zhang and Ming Yin</a></li>
        <li><a>Group 7: Osama Ahmed</a></li>
        <li><a>Group 8: Ryan Devries</a></li>
     </ul>



     04/16 &nbsp;&nbsp; <strong> Project presentation  </strong>
    <ul>
        <li><a>Group 9: Naman Saxena</a></li>
        <li><a>Group 10: Vivian Zhang</a></li>
        <li><a>Group 11: Anika Mitra and Adam Kosinski</a></li>
        <li><a>Group 12: Hao-Lun Hsu and Jiwoo Kim</a></li>
        <li><a>Group 13: Steven Seiden</a></li>
        <li><a>Group 14: Michael Li and Chengyang Zhou</a></li>
        <li><a>Group 15: Qinsi Wang and Ming-Yu Chung</a></li>
        <li><a>Group 16: Peter Yang</a></li>
    </ul>


<h2>
Prerequisite
</h2>

ECE 580 or 687D or Computer Science 371 or graduate standing.


<h2>
Course Description
</h2>
Generative AI is revolutionizing content creation by enabling machines to generate text, images, videos, music, and even code. In this course, we will discuss foundations, applications, and safety and security of generative AI.

<h2>
Class Format
</h2>
The class is structured around paper reading, lectures, discussions, and projects. Each lecture will focus on a specific topic, with students expected to read the suggested papers and submit their comments to a designated email address by the end of the day before the lecture. Students will be required to lead a lecture on a chosen topic, complete a class project, present their project, and write a project report. Groups of up to three students can be formed for both the lecture and the class project.
<h2>
Deadlines
</h2>
Reading assignments
<ul>
    <li>Sunday and Tuesday 11:59pm. Send comments to ecegenerativeai@gmail.com. Please send your comments to all papers in a single email thread.</li>
</ul>

Choosing a topic for lecture
<ul>
    <li>A group sends three preferred dates to ecegenerativeai@gmail.com by 11:59pm, 01/25.</li>
</ul>

Class project
<ul>
    <li>02/01: project proposal due.</li>
    <li>03/15: milestone report due.</li>
    <li>04/14, 04/16: project presentation.</li>
    <li>04/27: final project report due.</li>
</ul>


<h2>
Grading Policy
</h2>
50% project<br>
25% reading assignment<br>
10% class participation<br>
15% class presentation

    <br>
                        <br>
                        
                        <br>
                        <br>
                        
     </div>
    </body>
</html>

